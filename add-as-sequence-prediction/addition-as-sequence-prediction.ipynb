{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the blog \"Learn to Add Numbers with an Encoder-Decoder LSTM Recurrent Neural Network\" by Jason Brownlee.  \n",
    "https://machinelearningmastery.com/learn-add-numbers-seq2seq-recurrent-neural-networks/\n",
    "\n",
    "Reimplemented the code using Tensorflow in order to visualize the inner workings of the encoder and decoder RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation \n",
    "Copied the data generation scripts from the blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate examples of random integers and their sum\n",
    "def random_sum_pairs(n_examples, n_numbers, largest):\n",
    "    X, y = list(), list()\n",
    "    for i in range(n_examples):\n",
    "        in_pattern = [random.randint(1,largest) for _ in range(n_numbers)]\n",
    "        out_pattern = sum(in_pattern)\n",
    "        X.append(in_pattern)\n",
    "        y.append(out_pattern)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert data to strings\n",
    "def to_string(X, y, n_numbers, largest):\n",
    "    # input\n",
    "    max_length = n_numbers * math.ceil(math.log10(largest+1)) + n_numbers - 1\n",
    "    Xstr = list()\n",
    "    for pattern in X:\n",
    "        strp = '+'.join([str(n) for n in pattern])\n",
    "        # add padding to the left\n",
    "        strp = ''.join([' ' for _ in range(max_length-len(strp))]) + strp\n",
    "        Xstr.append(strp)\n",
    "    \n",
    "    # output\n",
    "    max_length = math.ceil(math.log10(n_numbers * (largest + 1)))\n",
    "    ystr = list()\n",
    "    for pattern in y:\n",
    "        strp = str(pattern)\n",
    "        strp = ''.join([' ' for _ in range(max_length-len(strp))]) + strp\n",
    "        ystr.append(strp)\n",
    "        \n",
    "    return Xstr, ystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# integer encode strings\n",
    "def integer_encode(X, y, alphabet):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    Xenc = list()\n",
    "    for pattern in X:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        Xenc.append(integer_encoded)\n",
    "    yenc = list()\n",
    "    \n",
    "    for pattern in y:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        yenc.append(integer_encoded)\n",
    "    \n",
    "    return Xenc, yenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode\n",
    "def one_hot_encode(X, y, max_int):\n",
    "    Xenc, yenc = list(), list()\n",
    "    \n",
    "    if (X is not None):\n",
    "        for seq in X:\n",
    "            pattern = list()\n",
    "            for index in seq:\n",
    "                vector = [0 for _ in range(max_int)]\n",
    "                vector[index] = 1\n",
    "                pattern.append(vector)\n",
    "            Xenc.append(pattern)\n",
    "    \n",
    "    if (y is not None):\n",
    "        for seq in y:\n",
    "            pattern = list()\n",
    "            for index in seq:\n",
    "                vector = [0 for _ in range(max_int)]\n",
    "                vector[index] = 1\n",
    "                pattern.append(vector)\n",
    "            yenc.append(pattern)\n",
    "    \n",
    "    return Xenc, yenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate an encoded dataset\n",
    "def generate_data(n_samples, n_numbers, largest, alphabet):\n",
    "    # generate pairs\n",
    "    X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "    # convert to strings\n",
    "    X, y = to_string(X, y, n_numbers, largest)\n",
    "    # integer encode\n",
    "    X, y = integer_encode(X, y, alphabet)\n",
    "    # one hot encode\n",
    "    X, y = one_hot_encode(X, y, len(alphabet))\n",
    "    # return as numpy as arrays\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# invert encoding\n",
    "def invert(seq, alphabet):\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    strings = list()\n",
    "    for pattern in seq:\n",
    "        string = int_to_char[np.argmax(pattern)]\n",
    "        strings.append(string)\n",
    "    return ''.join(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_batch = 10\n",
    "n_epoch = 30\n",
    "\n",
    "# Data parameters\n",
    "n_samples = 20\n",
    "n_numbers = 2\n",
    "largest = 30\n",
    "\n",
    "# Architecture parameters\n",
    "alphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', ' ']\n",
    "n_chars = len(alphabet)\n",
    "n_in_seq_length = n_numbers * math.ceil(math.log10(largest+1)) + n_numbers - 1\n",
    "n_out_seq_length = math.ceil(math.log10(n_numbers * (largest+1)))\n",
    "\n",
    "n_neurons_1 = 100\n",
    "n_neurons_2 = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "data = tf.placeholder(\"float\", [None, n_in_seq_length, n_chars])\n",
    "target = tf.placeholder(tf.int64, [None, n_out_seq_length, n_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create LSTM;  Use different scope for encoder and decoder.\n",
    "with tf.variable_scope('encoder'):\n",
    "    encoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons_1)\n",
    "    outputs1, states1 = tf.nn.dynamic_rnn(encoder_cell, data, dtype=tf.float32)\n",
    "    top_hidden_state = states1[1]\n",
    "\n",
    "# Make copies of encoded output. One for each output number placement. \n",
    "# ie. 12 has 2 digit position so create 2 copies of the encoded output.\n",
    "# The value is calculated and stored in the \"n_out_seq_length\".\n",
    "# Should function like the \"RepeatVector\" layer in Keras\n",
    "encoded_output = tf.tile(tf.expand_dims(top_hidden_state, axis=-2), \n",
    "                         [1, n_out_seq_length, 1])\n",
    "\n",
    "# Wrap each copy of the encoded output with Dense layer.\n",
    "# Should function like the \"TimeDistributed(Dense(...))\" in Keras\n",
    "with tf.variable_scope('decoder'):\n",
    "    decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons_2),\n",
    "        output_size=n_chars)\n",
    "    decoded_output, decoded_states = tf.nn.dynamic_rnn(\n",
    "        decoder_cell, encoded_output, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost Function\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss_op = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels = target,\n",
    "    logits = decoded_output\n",
    "))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epoch = 10\n",
    "batch = 100\n",
    "\n",
    "boolarr = []\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    boolarr = []\n",
    "    for epoch in range(n_epoch):\n",
    "        batch_x, batch_y = generate_data(n_samples, n_numbers, largest, alphabet)\n",
    "        for iteration in range(batch):\n",
    "            sess.run(train_op, feed_dict={data: batch_x, target: batch_y})\n",
    "        \n",
    "        output = sess.run(decoded_output, feed_dict={data: batch_x, target: batch_y})        \n",
    "        logits, _ = one_hot_encode(np.argmax(output, axis=2), None, len(alphabet))\n",
    "        \n",
    "        # make sure \"model\" folder is present\n",
    "        dirpath = './model'\n",
    "        ckpt = tf.train.get_checkpoint_state(dirpath)\n",
    "        if (ckpt is None):\n",
    "            saver.save(sess, dirpath + '/model.ckpt', global_step=epoch)\n",
    "        \n",
    "        #  Evaluate\n",
    "        for i in range(n_samples):\n",
    "            expected = invert(batch_y[i], alphabet)\n",
    "            predicted = invert(logits[i], alphabet)\n",
    "            correct = expected == predicted\n",
    "            boolarr.append(correct)\n",
    "\n",
    "    print('Accuracy: ', np.mean(boolarr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-0\n",
      "Accuracy:  0.922727272727\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state('./model/')\n",
    "if (ckpt is not None):\n",
    "    with tf.Session() as session:\n",
    "        saver.restore(session, ckpt.model_checkpoint_path)\n",
    "\n",
    "        X, y = generate_data(n_samples, n_numbers, largest, alphabet)\n",
    "        output = session.run(decoded_output, feed_dict={data: X, target: y})\n",
    "        logits, _ = one_hot_encode(np.argmax(output, axis=2), None, len(alphabet))\n",
    "\n",
    "        #  Evaluate\n",
    "        for i in range(n_samples):\n",
    "            expected = invert(y[i], alphabet)\n",
    "            predicted = invert(logits[i], alphabet)\n",
    "\n",
    "            correct = expected == predicted\n",
    "            boolarr.append(correct)\n",
    "\n",
    "        print('Accuracy: ', np.mean(boolarr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
